{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f88089e91b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform,DualTransform\n",
    "from tqdm.auto import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'background': 0,\n",
       " 'Load': 1,\n",
       " 'Sidewalk': 2,\n",
       " 'Construction': 3,\n",
       " 'Fence': 4,\n",
       " 'Pole': 5,\n",
       " 'Traffic Light': 6,\n",
       " 'Traffic Sign': 7,\n",
       " 'Nature': 8,\n",
       " 'Sky': 9,\n",
       " 'Person': 10,\n",
       " 'Rider': 11,\n",
       " 'Car': 12,\n",
       " 'Background': 13}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {\n",
    "    0:'background',\n",
    "    1:'Load',\n",
    "    2:'Sidewalk',\n",
    "    3:'Construction',\n",
    "    4:'Fence',\n",
    "    5:'Pole',\n",
    "    6:'Traffic Light',\n",
    "    7:'Traffic Sign',\n",
    "    8:'Nature',\n",
    "    9:'Sky',\n",
    "    10:'Person',\n",
    "    11:'Rider',\n",
    "    12:'Car',\n",
    "    13:'Background',\n",
    "}\n",
    "id2label_check = {\n",
    "    0:'background',\n",
    "    1:'Load',\n",
    "    2:'Sidewalk',\n",
    "    3:'Construction',\n",
    "    4:'Fence',\n",
    "    5:'Pole',\n",
    "    6:'Traffic Light',\n",
    "    7:'Traffic Sign',\n",
    "    8:'Nature',\n",
    "    9:'Sky',\n",
    "    10:'Person',\n",
    "    11:'Rider',\n",
    "    12:'Car',\n",
    "    13:'background_car',\n",
    "    14:'background_255',\n",
    "}\n",
    "label2id = {id2label[x]:x for x in id2label}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Mask2FormerForUniversalSegmentation\n",
    "from transformers import AutoImageProcessor\n",
    "model_checkpoint = 'last_epoch3'\n",
    "model_checkpoint = 'best_model_base3'\n",
    "\n",
    "# model_checkpoint = 'best_model'\n",
    "\n",
    "# model_checkpoint = 'last_epoch_g'\n",
    "# # model_name = 'checkpoint_final_74_new'\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(model_checkpoint,\n",
    "                                                          id2label=id2label,label2id=label2id,ignore_mismatched_sizes=True\n",
    "                                                          )\n",
    "model2 = Mask2FormerForUniversalSegmentation.from_pretrained(model_checkpoint,\n",
    "                                                          id2label=id2label,label2id=label2id,ignore_mismatched_sizes=True\n",
    "                                                          )\n",
    "# model = Mask2FormerForUniversalSegmentation.from_pretrained(model_checkpoint,ignore_mismatched_sizes=True)\n",
    "preprocessor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-large-cityscapes-semantic\",do_resize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor.size = {\n",
    "#     \"height\": 784,\n",
    "#     \"width\": 784\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "ADE_STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_path:list,mask_path:list, transform=None, infer=False):\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "        self.img_path=image_path\n",
    "        self.mask_path=mask_path\n",
    "    def __len__(self):\n",
    "        # return len(self.data)\n",
    "        return len(self.img_path)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_path[idx]\n",
    "        image = cv2.imread(f\"{img_path}\")\n",
    "        ori_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=ori_image)['image']\n",
    "            return image,ori_image\n",
    "        mask_path = self.mask_path[idx]\n",
    "        ori_mask = cv2.imread(f\"{mask_path}\", cv2.IMREAD_GRAYSCALE)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=ori_image, mask=ori_mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "            if mask.size()==0 or image.size()==0:\n",
    "                print(\"ERROR READ IMAGE FAILE\")\n",
    "        return image, mask ,ori_image,ori_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs = list(zip(*batch))\n",
    "    images = inputs[0]\n",
    "    original_images = inputs[1]\n",
    "    \n",
    "    batch = preprocessor.preprocess(\n",
    "        images,\n",
    "        ignore_index = 255,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    batch[\"original_images\"] = original_images\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_transform = A.Compose([\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1898"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_glob= glob('./test_image/*.png')\n",
    "sub_glob.sort()\n",
    "sub_dataset = CustomDataset(sub_glob, [], transform=sub_transform, infer=True)\n",
    "print(len(sub_dataset))\n",
    "sub_dataloader = DataLoader(sub_dataset, batch_size=1, shuffle=False,collate_fn=collate_fn)\n",
    "len(sub_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04514d436a842ae8f148d295209db4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model2.to(device)\n",
    "model.eval()\n",
    "result = []\n",
    "fig = plt.figure()\n",
    "rows = 15\n",
    "cols = 15\n",
    "i = 1\n",
    "for idx, batch in enumerate(tqdm(sub_dataloader)):\n",
    "\n",
    "  # print(batch.shape)\n",
    "  pixel_values = batch['pixel_values']\n",
    "\n",
    "  # Forward pass\n",
    "  with torch.no_grad():\n",
    "    outputs = model(pixel_values=pixel_values.to(device))\n",
    "    outputs2 = model2(pixel_values=pixel_values.to(device))\n",
    "  # get original images \n",
    "    original_images = batch['original_images']\n",
    "    target_sizes = [(image.shape[0], image.shape[1]) for image in original_images]\n",
    "    # predict segmentation maps\n",
    "    predicted_segmentation_maps = preprocessor.post_process_semantic_segmentation(outputs,target_sizes=target_sizes)\n",
    "    predicted_segmentation_maps2 = preprocessor.post_process_semantic_segmentation(outputs2,target_sizes=target_sizes)\n",
    "    \n",
    "    for pred,pred2 in zip(predicted_segmentation_maps,predicted_segmentation_maps2):\n",
    "        pred = pred.cpu().numpy()\n",
    "        pred2 = pred2.cpu().numpy()\n",
    "        # pred = cv2.copyTo(pred,mask_valid)\n",
    "        pred = pred.astype(np.uint8)\n",
    "        pred2 = pred2.astype(np.uint8)\n",
    "        pred = Image.fromarray(pred) # 이미지로 변환\n",
    "        pred2 = Image.fromarray(pred2)\n",
    "        pred = pred.resize((960, 540), Image.NEAREST) # 960 x 540 사이즈로 변환\n",
    "        pred2 = pred2.resize((960, 540), Image.NEAREST)\n",
    "        pred = np.array(pred) # 다시 수치로 변환\n",
    "        pred2 = np.array(pred2) \n",
    "        # class 0 ~ 11에 해당하는 경우에 마스크 형성 / 12(배경)는 제외하고 진행\n",
    "        pred[pred2==5]=5\n",
    "        pred[pred2==4]=4\n",
    "        pred[pred2==10]=10\n",
    "        pred[pred2==13]=13\n",
    "        pred[pred2==0]=0\n",
    "        for class_id in range(1,13):\n",
    "            class_mask = (pred == class_id).astype(np.uint8)\n",
    "            if np.sum(class_mask) > 0: # 마스크가 존재하는 경우 encode\n",
    "                mask_rle = rle_encode(class_mask)\n",
    "                result.append(mask_rle)\n",
    "            else: # 마스크가 존재하지 않는 경우 -1  \n",
    "                result.append(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>mask_rle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000_class_0</td>\n",
       "      <td>210709 3 211668 5 212607 10 212627 14 212652 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0000_class_1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0000_class_2</td>\n",
       "      <td>598 273 1557 275 2516 276 3476 276 4436 277 53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0000_class_3</td>\n",
       "      <td>201964 6 202916 26 203876 45 204837 74 205798 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0000_class_4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22771</th>\n",
       "      <td>TEST_1897_class_7</td>\n",
       "      <td>151289 5 152246 12 152270 13 153205 14 153226 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22772</th>\n",
       "      <td>TEST_1897_class_8</td>\n",
       "      <td>101 535 676 128 851 23 1061 534 1637 125 1811 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22773</th>\n",
       "      <td>TEST_1897_class_9</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22774</th>\n",
       "      <td>TEST_1897_class_10</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22775</th>\n",
       "      <td>TEST_1897_class_11</td>\n",
       "      <td>191432 3 192378 21 193335 30 194293 33 195253 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22776 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                           mask_rle\n",
       "0       TEST_0000_class_0  210709 3 211668 5 212607 10 212627 14 212652 3...\n",
       "1       TEST_0000_class_1                                                 -1\n",
       "2       TEST_0000_class_2  598 273 1557 275 2516 276 3476 276 4436 277 53...\n",
       "3       TEST_0000_class_3  201964 6 202916 26 203876 45 204837 74 205798 ...\n",
       "4       TEST_0000_class_4                                                 -1\n",
       "...                   ...                                                ...\n",
       "22771   TEST_1897_class_7  151289 5 152246 12 152270 13 153205 14 153226 ...\n",
       "22772   TEST_1897_class_8  101 535 676 128 851 23 1061 534 1637 125 1811 ...\n",
       "22773   TEST_1897_class_9                                                 -1\n",
       "22774  TEST_1897_class_10                                                 -1\n",
       "22775  TEST_1897_class_11  191432 3 192378 21 193335 30 194293 33 195253 ...\n",
       "\n",
       "[22776 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['mask_rle'] = result\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./baseline_submit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "11ad4dcd66484cb2927de752e61ef314": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1603118d0c6744148c8e60f5fe364b9d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2512bdff89db40fd83b54ef265a31802": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2c08f94fdad04ab5848a10ff415997de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "3acd4f52261f4c42a87b89d3a7c11937": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3b1bdfc187604ebf9fce4c0d4532b7ca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "48bef1e4e5ed4c79bbbcaaa87d020f25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "572fa3de5a9741eaa37634653431f244": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "70b69e33570f485d80681bc18ae9a9aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_74bb4409a1af4aa89c4a8b9c5c81b504",
       "style": "IPY_MODEL_572fa3de5a9741eaa37634653431f244",
       "value": "100%"
      }
     },
     "74b9ee18b9444a2f9fef22458a93d01c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_3b1bdfc187604ebf9fce4c0d4532b7ca",
       "max": 1898,
       "style": "IPY_MODEL_2c08f94fdad04ab5848a10ff415997de",
       "value": 1898
      }
     },
     "74bb4409a1af4aa89c4a8b9c5c81b504": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "80e21ed47af14acc906d4627591d3584": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2512bdff89db40fd83b54ef265a31802",
       "style": "IPY_MODEL_de5c6574da864a7d91b1bc9cd63396f0",
       "value": "  0%"
      }
     },
     "820ffecf5e704202a234ac1fdf204441": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "89a7c119cf124ed986be69da71727e73": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_cb4c5044570b4ba1b8b34b06222665a0",
       "style": "IPY_MODEL_48bef1e4e5ed4c79bbbcaaa87d020f25",
       "value": " 0/1898 [00:02&lt;?, ?it/s]"
      }
     },
     "a04514d436a842ae8f148d295209db4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_70b69e33570f485d80681bc18ae9a9aa",
        "IPY_MODEL_74b9ee18b9444a2f9fef22458a93d01c",
        "IPY_MODEL_d0f11d6268e247408feadf04079ffe63"
       ],
       "layout": "IPY_MODEL_1603118d0c6744148c8e60f5fe364b9d"
      }
     },
     "a33ad74a72cb425c8621bd3109bd56d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_80e21ed47af14acc906d4627591d3584",
        "IPY_MODEL_fbdda152f4194f7397a27649e646d24d",
        "IPY_MODEL_89a7c119cf124ed986be69da71727e73"
       ],
       "layout": "IPY_MODEL_820ffecf5e704202a234ac1fdf204441"
      }
     },
     "c5f2f1c3ef4c4395904b4531a0ebef22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "cb4c5044570b4ba1b8b34b06222665a0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d0f11d6268e247408feadf04079ffe63": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_fe82d6b3b8b14a3398ca0e8df6dee2e6",
       "style": "IPY_MODEL_3acd4f52261f4c42a87b89d3a7c11937",
       "value": " 1898/1898 [1:23:52&lt;00:00,  2.83s/it]"
      }
     },
     "de5c6574da864a7d91b1bc9cd63396f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fbdda152f4194f7397a27649e646d24d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "danger",
       "layout": "IPY_MODEL_11ad4dcd66484cb2927de752e61ef314",
       "max": 1898,
       "style": "IPY_MODEL_c5f2f1c3ef4c4395904b4531a0ebef22"
      }
     },
     "fe82d6b3b8b14a3398ca0e8df6dee2e6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
